{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import subprocess\n",
    "\n",
    "# AWS Configuration\n",
    "s3_bucket = 'your-s3-bucket'\n",
    "pipeline_name = 'your-pipeline-name'\n",
    "\n",
    "# Boto3 clients\n",
    "s3_client = boto3.client('s3')\n",
    "dp_client = boto3.client('datapipeline')\n",
    "cloudwatch_client = boto3.client('cloudwatch')\n",
    "\n",
    "\n",
    "# SQL Server Configuration\n",
    "tables = ['table1', 'table2']  # List all table names\n",
    "sql_server_connection = 'your-sql-connection-string'\n",
    "\n",
    "def create_sql_scripts_for_large_table(table_name, chunk_size):\n",
    "    scripts = []\n",
    "    # Example: Assuming you have an ID column for chunking\n",
    "    last_id = 0\n",
    "\n",
    "    while True:\n",
    "        script = f\"\"\"\n",
    "        bcp \"SELECT * FROM {table_name} WHERE id > {last_id} AND id <= {last_id + chunk_size}\" queryout s3://{s3_bucket}/{table_name}_{last_id}.csv -c -t, -r\\\\n -U username -P password\n",
    "        \"\"\"\n",
    "        script_path = f\"/tmp/{table_name}_{last_id}.sql\"\n",
    "        \n",
    "        with open(script_path, 'w') as file:\n",
    "            file.write(script)\n",
    "        scripts.append(script_path)\n",
    "        last_id += chunk_size\n",
    "        # Check if there are more records (you need to implement this function based on your needs)\n",
    "        if not has_more_records(last_id):\n",
    "            break\n",
    "    return scripts\n",
    "\n",
    "def upload_scripts_to_s3(script_paths, table_name):\n",
    "    for script_path in script_paths:\n",
    "        script_name = script_path.split('/')[-1]  # Extract the script name from the path\n",
    "        s3_key = f'scripts/{table_name}/{script_name}'\n",
    "        s3_client.upload_file(script_path, s3_bucket, s3_key)\n",
    "        print(f\"Uploaded {script_path} to s3://{s3_bucket}/{s3_key}\")\n",
    "\n",
    "\n",
    "def list_sql_files_in_s3(table_name):\n",
    "    prefix = f'scripts/{table_name}/'\n",
    "    response = s3_client.list_objects_v2(Bucket=s3_bucket, Prefix=prefix)\n",
    "    files = [obj['Key'] for obj in response.get('Contents', []) if obj['Key'].endswith('.sql')]\n",
    "    # example full list output would be \n",
    "    # ['scripts/table1/table1_0.sql', 'scripts/table1/table1_1000.sql', 'scripts/table1/table1_2000.sql', ...]\n",
    "    return files\n",
    "\n",
    "def create_pipeline_definition(table_name):\n",
    "    sql_files = list_sql_files_in_s3(table_name) # List all SQL files for the table which is the pattern table_name_chunkId.sql Ex: table1_0.sql, table1_1000.sql, ... \n",
    "    activities = []\n",
    "\n",
    "    for file in sql_files:\n",
    "        # file 'scripts/table1/table1_0.sql'\n",
    "        chunk_id = file.split('_')[-1].split('.')[0]  # Extract chunkId from filename\n",
    "        # file.split('_')[-1] would be '0.sql' and file.split('_')[-1].split('.')[0] would be '0'\n",
    "        # chunk_id = 0 since the file is table1_0.sql abd chunkId is 0 \n",
    "        activity = {\n",
    "            \"id\": f\"SqlTableCopyActivity_{chunk_id}\",\n",
    "            \"name\": f\"SqlTableCopyActivity_{chunk_id}\",\n",
    "            \"type\": \"SqlActivity\",\n",
    "            \"fields\": [\n",
    "                {\"key\": \"runsOn\", \"refValue\": \"Ec2Resource\"},\n",
    "                {\"key\": \"type\", \"stringValue\": \"Copy\"},\n",
    "                {\"key\": \"scriptUri\", \"stringValue\": f\"s3://{s3_bucket}/{file}\"},\n",
    "                {\"key\": \"mySqlDatabase\", \"stringValue\": \"jdbc:mysql://your-db-endpoint\"},\n",
    "                {\"key\": \"username\", \"stringValue\": \"your-db-username\"},\n",
    "                {\"key\": \"password\", \"stringValue\": \"your-db-password\"},\n",
    "                {\"key\": \"table\", \"stringValue\": table_name}\n",
    "            ]\n",
    "        }\n",
    "        activities.append(activity)\n",
    "\n",
    "        \n",
    "     pipeline_definition = {\n",
    "        \"objects\": [\n",
    "            {\n",
    "                \"id\": \"Default\",\n",
    "                \"name\": \"Default\",\n",
    "                \"fields\": [\n",
    "                    {\"key\": \"failureAndRerunMode\", \"stringValue\": \"CASCADE\"},\n",
    "                    {\"key\": \"role\", \"stringValue\": \"DataPipelineDefaultRole\"},\n",
    "                    {\"key\": \"resourceRole\", \"stringValue\": \"DataPipelineDefaultResourceRole\"},\n",
    "                    {\"key\": \"pipelineLogUri\", \"stringValue\": \"s3://your-log-bucket/\"},\n",
    "                    {\"key\": \"scheduleType\", \"stringValue\": \"cron\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"Ec2Resource\",\n",
    "                \"name\": \"Ec2Resource\",\n",
    "                \"type\": \"Ec2Resource\",\n",
    "                \"fields\": [\n",
    "                    {\"key\": \"instanceType\", \"stringValue\": \"m5.large\"},\n",
    "                    {\"key\": \"imageId\", \"stringValue\": \"ami-0c55b159cbfafe1f0\"}\n",
    "                ]\n",
    "            }\n",
    "        ] + activities,\n",
    "        \"parameters\": [\n",
    "            {\n",
    "                \"id\": \"myTable\",\n",
    "                \"type\": \"String\",\n",
    "                \"description\": \"Table to copy\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    return pipeline_definition\n",
    "\n",
    "def create_and_activate_pipeline(table_name):\n",
    "    pipeline_definition = create_pipeline_definition(table_name)\n",
    "    response = dp_client.create_pipeline(\n",
    "        name=pipeline_name,\n",
    "        uniqueId=f\"{pipeline_name}-{table_name}\"\n",
    "    )\n",
    "    pipeline_id = response['pipelineId']\n",
    "    \n",
    "    dp_client.put_pipeline_definition( # Put the pipeline definition with the activities and parameters \n",
    "        pipelineId=pipeline_id,\n",
    "        pipelineObjects=pipeline_definition['objects'],\n",
    "        parameterObjects=pipeline_definition['parameters']\n",
    "    )\n",
    "    \n",
    "    dp_client.activate_pipeline(pipelineId=pipeline_id)\n",
    "    print(f\"Activated pipeline for table: {table_name}\")\n",
    "    # Here dp_client is the boto3 client for DataPipeline service \n",
    "\n",
    "\n",
    "def main():\n",
    "    for table in tables:\n",
    "        try:\n",
    "            # Log start of migration\n",
    "            print(f\"Starting migration for table: {table}\")\n",
    "\n",
    "            # Create SQL scripts\n",
    "            script_paths = create_sql_scripts_for_large_table(table, chunk_size=1000)\n",
    "            print(f\"Created {len(script_paths)} SQL scripts for table: {table}\")\n",
    "\n",
    "            # Upload scripts to S3\n",
    "            upload_scripts_to_s3(script_paths, table)\n",
    "            print(f\"Uploaded SQL scripts to S3 for table: {table}\")\n",
    "\n",
    "            # Create and activate the pipeline\n",
    "            create_and_activate_pipeline(table)\n",
    "            print(f\"Activated pipeline for table: {table}\")\n",
    "\n",
    "            print(f\"Completed migration for table: {table}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Log any errors encountered\n",
    "            print(f\"Error occurred while migrating table {table}: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify create_sql_scripts_for_large_table Function\n",
    "You need to add logic to track and fetch only the new or updated records. This requires maintaining a record of the last processed timestamp or primary key.\n",
    "\n",
    "Changes Needed:\n",
    "\n",
    "Track Last Processed Record: Store the last processed primary key or timestamp.\n",
    "Fetch New Records: Adjust the SQL script to fetch records newer than the last processed timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the data sizes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "\n",
    "# AWS Configuration\n",
    "s3_bucket = 'your-s3-bucket'\n",
    "\n",
    "# SQL Server Configuration\n",
    "sql_server_connection = 'your-sql-connection-string'\n",
    "tables = ['table1', 'table2']\n",
    "\n",
    "# Boto3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "def get_record_count_from_sql_server(table_name):\n",
    "    \"\"\" Get record count from SQL Server \"\"\"\n",
    "    conn = pyodbc.connect(sql_server_connection)\n",
    "    query = f\"SELECT COUNT(*) FROM {table_name}\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "    count = cursor.fetchone()[0]\n",
    "    conn.close()\n",
    "    return count\n",
    "\n",
    "def get_record_count_from_s3(table_name):\n",
    "    \"\"\" Get record count from S3 by summing up counts from all CSV files \"\"\"\n",
    "    s3_key_prefix = f'data/{table_name}/'\n",
    "    response = s3_client.list_objects_v2(Bucket=s3_bucket, Prefix=s3_key_prefix)\n",
    "    total_count = 0\n",
    "    \n",
    "    if 'Contents' in response:\n",
    "        for obj in response['Contents']:\n",
    "            s3_key = obj['Key']\n",
    "            if s3_key.endswith('.csv'):\n",
    "                response = s3_client.get_object(Bucket=s3_bucket, Key=s3_key)\n",
    "                df = pd.read_csv(response['Body'])\n",
    "                total_count += len(df)\n",
    "    \n",
    "    return total_count\n",
    "\n",
    "def compare_counts():\n",
    "    \"\"\" Compare record counts between SQL Server and S3 \"\"\"\n",
    "    discrepancies = []\n",
    "    \n",
    "    for table in tables:\n",
    "        try:\n",
    "            # Get record counts\n",
    "            sql_count = get_record_count_from_sql_server(table)\n",
    "            s3_count = get_record_count_from_s3(table)\n",
    "            \n",
    "            # Compare counts\n",
    "            if sql_count != s3_count:\n",
    "                discrepancies.append({\n",
    "                    'table': table,\n",
    "                    'sql_count': sql_count,\n",
    "                    's3_count': s3_count\n",
    "                })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing table {table}: {e}\")\n",
    "    \n",
    "    return discrepancies\n",
    "\n",
    "def main():\n",
    "    discrepancies = compare_counts()\n",
    "    \n",
    "    if discrepancies:\n",
    "        print(\"Discrepancies found:\")\n",
    "        for item in discrepancies:\n",
    "            print(f\"Table: {item['table']}, SQL Server Count: {item['sql_count']}, S3 Count: {item['s3_count']}\")\n",
    "    else:\n",
    "        print(\"No discrepancies found. All counts match.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
